{"nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "source": "%%configure -f\n{ \"conf\": {\"spark.jars.packages\": \"com.databricks:spark-avro_2.11:4.0.0\" }}", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'pyspark3', u'conf': {u'spark.jars.packages': u'com.databricks:spark-avro_2.11:4.0.0'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "No active sessions."}, "metadata": {}}], "metadata": {"cell_status": {"execute_time": {"duration": 30.174072265625, "end_time": 1557664032371.455}}, "collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": "from pyspark.sql import SparkSession\nimport re\n\nspark = SparkSession \\\n    .builder \\\n    .appName(\"spark-avro-json-sample\") \\\n    .config('spark.hadoop.avro.mapred.ignore.inputs.without.extension', 'false') \\\n    .getOrCreate()\n\n\n#main code\n\npath = \"wasbs://github@renjie.blob.core.chinacloudapi.cn/renjie/renjieeh/*/2019/05/*/*/*/46.avro\"\navroDf = spark.read.format(\"com.databricks.spark.avro\").option(\"mergeSchema\", \"true\").load(path)\n        \n#avro->json\njsonRdd = avroDf.select(avroDf.Body.cast(\"string\")).rdd.map(lambda x: (\"[\" + re.sub(\"}\\n{\\\"id\\\"\", \"},{\\\"id\\\"\", str(x[0])) + \"]\"))\ndatax = spark.read.json(jsonRdd)\n\n\n\ndatawatch = datax.filter(\"type == 'WatchEvent'\").groupby(\"Repo.name\").count() \ndataevent = datax.filter(\"type == 'IssuesEvent' OR type == 'IssueCommentEvent'\").groupby(\"Repo.name\").count()\n\ndatajoin = datawatch.selectExpr(\"name as name\", \"count as countwatch\")\ndataevent = dataevent.selectExpr(\"name as name\", \"count as countevent\")\n\n\ndatajoin = datajoin.join(dataevent, \"name\", 'outer')\n\n\n\nfrom pyspark.sql.functions import udf,col\nfrom pyspark.sql.types import LongType\ndef func(s,a):\n    if s != None:\n        s = s * 3\n    elif s == None:\n        s = 0\n    else:\n        s = 0\n    \n    if a != None:\n        a = a * 7\n    elif a == None:\n        a = 0\n    else:\n        a = 0\n    \n    r = a + s\n    return r\nmy_func = udf(func,LongType())\ndatajoin = datajoin.withColumn('index', my_func('countwatch','countevent'))\n\n\ndatax = datajoin.sort(\"index\", ascending=False)\n\n\n#Plot barchart for popularity index\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndataplot = datax.select(\"name\", \"index\").collect()\nname = [item[0] for item in dataplot]\nindex = [item[1] for item in dataplot]\ndataplot = {\"name\" : name, \"index\": index}\ndataplot = pd.DataFrame(dataplot)\ndataplot = dataplot.iloc[:20]\n\nmyplot = dataplot.plot(figsize = (20,20), kind = \"barh\", color = \"#b35900\", width = 0.8,\n                               x = \"name\", y = \"index\", legend = False)\n\nmyplot.invert_yaxis()\nplt.xlabel(\"Number of Project\", fontsize = 28)\nplt.ylabel(\"Name of Project\", fontsize = 28)\nplt.title(\"Name of Project by number\", fontsize = 36)\nplt.xticks(size = 24)\nplt.yticks(size = 24)\n\nplt.show()\n", "outputs": [{"output_type": "display_data", "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "28444acf900a4128893ae60e82348acd"}}, "metadata": {}}], "metadata": {"cell_status": {"execute_time": {"duration": 39871.446044921875, "end_time": 1557664565231.852}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}}}